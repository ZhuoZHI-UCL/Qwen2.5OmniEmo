# === model ===
# model_name_or_path:  "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni/hf_cache/Qwen2.5-Omni-7B"
# processor_name_or_path: "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni/hf_cache/Qwen2.5-Omni-7B" 
model_name_or_path:  "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00"  # 也可以直接用huggingface的模型名
trust_remote_code: false
use_audio_in_video: True
video_fps: 5 #这个对应的是0.4s,如果修改fps请同步修改hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00/config.json 中的 'seconds_per_chunk' 
video_maxlen: 300
video_max_pixels: 36864 # 192*192 = 36864    160*160 = 25600   256*256=65536
tokenized_path: "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/saves/datasets/lemon_tok_192_1sample" #这个要和video_max_pixels中的值对应
# === method ===
stage: sft
do_train: true
finetuning_type: lora           # 全参数微调 full 
print_param_status: true
# === 如果使用lora ===
# 下面是常用且稳健的默认值，可按需微调
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1
freeze_vision_tower: true
# === dataset ===
dataset: lemon                # 与 dataset_info.json 的键一致
template: qwen2_omni            # 官方推荐模板名
cutoff_len: 8192 #这个如果太小了会报错：Sizes of tensors must match except in dimension 1. Expected size 62 but got size 61 for tensor number 1 in the list.
overwrite_cache: true
preprocessing_num_workers: 8
streaming: false
dataloader_drop_last: true
# === output ===
output_dir: "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora_test" #lemon_omni_full
logging_steps: 5
save_steps: 100
save_total_limit: 1
plot_loss: false
overwrite_output_dir: false
# === train ===
use_cache: false
# deepspeed: "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/ds_zero2_bf16.json"
dataloader_pin_memory: false
max_steps: 201
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2.0e-5
# num_train_epochs: 10
lr_scheduler_type: cosine
warmup_ratio: 0.03
bf16: true
tf32: true
ddp_timeout: 180000000
gradient_checkpointing: true
seed: 42
report_to: wandb
# === eval === 
# do_eval: false #先给关掉，不然很慢
# eval_dataset: lemon 
# eval_strategy: "steps"
# eval_steps: 2
# per_device_eval_batch_size: 1         # 从 8 降到 1，最直接消峰
# eval_accumulation_steps: 32
# predict_with_generate: false          # 评估只算 loss，不做生成
# load_best_model_at_end: true #训练结束后自动加载在验证集上指标最优的 checkpoint
# metric_for_best_model: "eval_loss"
# greater_is_better: false




