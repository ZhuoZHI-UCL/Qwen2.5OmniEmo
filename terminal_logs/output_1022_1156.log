[INFO|2025-10-22 11:56:45] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:37237
W1022 11:56:47.114000 3091730 site-packages/torch/distributed/run.py:766] 
W1022 11:56:47.114000 3091730 site-packages/torch/distributed/run.py:766] *****************************************
W1022 11:56:47.114000 3091730 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 11:56:47.114000 3091730 site-packages/torch/distributed/run.py:766] *****************************************
[WARNING|2025-10-22 11:56:52] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-10-22 11:56:52,702] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-22 11:56:52,772] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-22 11:56:52,881] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-22 11:56:53,106] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-22 11:56:54,070] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-22 11:56:54,354] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-22 11:56:54,491] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-22 11:56:54,768] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-22 11:56:55,245] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-10-22 11:56:55,637] [INFO] [comm.py:821:init_distributed] cdb=None
[INFO|2025-10-22 11:56:55] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[2025-10-22 11:56:55,827] [INFO] [comm.py:821:init_distributed] cdb=None
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|2025-10-22 11:56:56] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-22 11:56:56] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[2025-10-22 11:56:56,268] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-10-22 11:56:56,268] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[INFO|2025-10-22 11:56:56] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-10-22 11:56:56] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,400 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,400 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,400 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,400 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,400 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,400 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,400 >> loading file chat_template.jinja
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|tokenization_utils_base.py:2299] 2025-10-22 11:56:56,806 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-10-22 11:56:56,807 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-10-22 11:56:56,817 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-10-22 11:56:56,817 >> Image processor Qwen2VLImageProcessor {
  "chunk_length": 300,
  "dither": 0.0,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[WARNING|logging.py:328] 2025-10-22 11:56:56,818 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|video_processing_utils.py:627] 2025-10-22 11:56:56,818 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00/preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-10-22 11:56:56,819 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "chunk_length": 300,
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "dither": 0.0,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|feature_extraction_utils.py:548] 2025-10-22 11:56:56,823 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00/preprocessor_config.json
[INFO|feature_extraction_utils.py:597] 2025-10-22 11:56:56,824 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 300,
  "dither": 0.0,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,825 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,825 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,825 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,825 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,825 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,825 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-10-22 11:56:56,825 >> loading file chat_template.jinja
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
Qwen2_5OmniToken2WavModel does not support eager attention implementation, fall back to sdpa
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s][INFO|tokenization_utils_base.py:2299] 2025-10-22 11:56:57,238 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Qwen2_5OmniToken2WavModel does not support eager attention implementation, fall back to sdpa
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s][INFO|processing_utils.py:990] 2025-10-22 11:56:57,615 >> Processor Qwen2_5OmniProcessor:
- image_processor: Qwen2VLImageProcessor {
  "chunk_length": 300,
  "dither": 0.0,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "chunk_length": 300,
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "dither": 0.0,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

- feature_extractor: WhisperFeatureExtractor {
  "chunk_length": 300,
  "dither": 0.0,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|AUDIO|>', '<|audio_bos|>', '<|audio_eos|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_bos|>', '<|vision_eos|>', '<|vision_pad|>', '<|IMAGE|>', '<|VIDEO|>'], 'image_token': '<|IMAGE|>', 'audio_token': '<|AUDIO|>', 'video_token': '<|VIDEO|>', 'vision_bos_token': '<|vision_bos|>', 'vision_eos_token': '<|vision_eos|>', 'audio_bos_token': '<|audio_bos|>', 'audio_eos_token': '<|audio_eos|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|AUDIO|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|audio_bos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|audio_eos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_bos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_eos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|IMAGE|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|VIDEO|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)

{
  "processor_class": "Qwen2_5OmniProcessor"
}

Qwen2_5OmniToken2WavModel does not support eager attention implementation, fall back to sdpa
[WARNING|2025-10-22 11:56:57] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
[INFO|2025-10-22 11:56:57] llamafactory.data.loader:143 >> Loaded tokenized dataset from /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/saves/datasets/lemon_tok_192.
[INFO|configuration_utils.py:696] 2025-10-22 11:56:57,656 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00/config.json
[WARNING|modeling_rope_utils.py:467] 2025-10-22 11:56:57,657 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|configuration_qwen2_5_omni.py:1031] 2025-10-22 11:56:57,659 >> thinker_config is None. Initializing thinker model with default values
[INFO|configuration_qwen2_5_omni.py:1035] 2025-10-22 11:56:57,659 >> talker_config is None. Initializing talker model with default values
[INFO|configuration_qwen2_5_omni.py:1039] 2025-10-22 11:56:57,659 >> token2wav_config is None. Initializing token2wav model with default values
[INFO|configuration_utils.py:770] 2025-10-22 11:56:57,663 >> Model config Qwen2_5OmniConfig {
  "architectures": [
    "Qwen2_5OmniForConditionalGeneration"
  ],
  "enable_audio_output": true,
  "enable_talker": true,
  "model_type": "qwen2_5_omni",
  "talker_config": {
    "_name_or_path": "Qwen2.5-Omni-7B/talker",
    "architectures": [
      "Qwen2OmniTalkerForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "audio_end_token_id": 151648,
    "audio_start_token_id": 151647,
    "audio_token_index": 151646,
    "embedding_size": 3584,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 896,
    "image_token_index": 151655,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_5_omni_talker",
    "num_attention_heads": 12,
    "num_hidden_layers": 24,
    "num_key_value_heads": 4,
    "position_id_per_seconds": 25,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "seconds_per_chunk": 0.4,
    "sliding_window": 32768,
    "spatial_merge_size": 2,
    "torch_dtype": "bfloat16",
    "tts_codec_end_token_id": 8294,
    "tts_codec_mask_token_id": 8296,
    "tts_codec_pad_token_id": 8292,
    "tts_codec_start_token_id": 8293,
    "tts_text_end_token_id": 151861,
    "tts_text_pad_token_id": 151859,
    "tts_text_start_token_id": 151860,
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_index": 151656,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vocab_size": 8448
  },
  "thinker_config": {
    "_name_or_path": "Qwen2.5-Omni-7B/thinker",
    "architectures": [
      "Qwen2OmniNaViTThinkerForConditionalGeneration"
    ],
    "audio_config": {
      "_name_or_path": "",
      "activation_dropout": 0.0,
      "activation_function": "gelu",
      "add_cross_attention": false,
      "architectures": null,
      "attention_dropout": 0.0,
      "bad_words_ids": null,
      "begin_suppress_tokens": null,
      "bos_token_id": null,
      "chunk_size_feed_forward": 0,
      "cross_attention_hidden_size": null,
      "d_model": 1280,
      "decoder_start_token_id": null,
      "diversity_penalty": 0.0,
      "do_sample": false,
      "dropout": 0.0,
      "early_stopping": false,
      "encoder_attention_heads": 20,
      "encoder_ffn_dim": 5120,
      "encoder_layerdrop": 0.0,
      "encoder_layers": 32,
      "encoder_no_repeat_ngram_size": 0,
      "eos_token_id": null,
      "exponential_decay_length_penalty": null,
      "finetuning_task": null,
      "forced_bos_token_id": null,
      "forced_eos_token_id": null,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "init_std": 0.02,
      "initializer_range": 0.02,
      "is_decoder": false,
      "is_encoder_decoder": false,
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "length_penalty": 1.0,
      "max_length": 20,
      "max_source_positions": 1500,
      "min_length": 0,
      "model_type": "qwen2_5_omni_audio_encoder",
      "n_window": 100,
      "no_repeat_ngram_size": 0,
      "num_beam_groups": 1,
      "num_beams": 1,
      "num_hidden_layers": 32,
      "num_mel_bins": 128,
      "num_return_sequences": 1,
      "output_attentions": false,
      "output_dim": 3584,
      "output_hidden_states": false,
      "output_scores": false,
      "pad_token_id": null,
      "prefix": null,
      "problem_type": null,
      "pruned_heads": {},
      "remove_invalid_values": false,
      "repetition_penalty": 1.0,
      "return_dict": true,
      "return_dict_in_generate": false,
      "scale_embedding": false,
      "sep_token_id": null,
      "suppress_tokens": null,
      "task_specific_params": null,
      "temperature": 1.0,
      "tf_legacy_loss": false,
      "tie_encoder_decoder": false,
      "tie_word_embeddings": true,
      "tokenizer_class": null,
      "top_k": 50,
      "top_p": 1.0,
      "torch_dtype": null,
      "torchscript": false,
      "typical_p": 1.0,
      "use_bfloat16": false
    },
    "audio_end_token_id": 151648,
    "audio_start_token_id": 151647,
    "audio_token_index": 151646,
    "bos_token_id": 151644,
    "eos_token_id": 151645,
    "ignore_index": -100,
    "image_token_index": 151655,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "model_type": "qwen2_5_omni_thinker",
    "pad_token_id": 151643,
    "position_id_per_seconds": 25,
    "seconds_per_chunk": 0.4,
    "text_config": {
      "_name_or_path": "",
      "add_cross_attention": false,
      "architectures": null,
      "attention_dropout": 0.0,
      "bad_words_ids": null,
      "begin_suppress_tokens": null,
      "bos_token_id": null,
      "chunk_size_feed_forward": 0,
      "cross_attention_hidden_size": null,
      "decoder_start_token_id": null,
      "diversity_penalty": 0.0,
      "do_sample": false,
      "early_stopping": false,
      "encoder_no_repeat_ngram_size": 0,
      "eos_token_id": null,
      "exponential_decay_length_penalty": null,
      "finetuning_task": null,
      "forced_bos_token_id": null,
      "forced_eos_token_id": null,
      "hidden_act": "silu",
      "hidden_size": 3584,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "init_std": 0.02,
      "initializer_range": 0.02,
      "intermediate_size": 18944,
      "is_decoder": false,
      "is_encoder_decoder": false,
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "length_penalty": 1.0,
      "max_length": 20,
      "max_position_embeddings": 32768,
      "max_window_layers": 28,
      "min_length": 0,
      "model_type": "qwen2_5_omni_text",
      "no_repeat_ngram_size": 0,
      "num_attention_heads": 28,
      "num_beam_groups": 1,
      "num_beams": 1,
      "num_hidden_layers": 28,
      "num_key_value_heads": 4,
      "num_return_sequences": 1,
      "output_attentions": false,
      "output_hidden_states": false,
      "output_scores": false,
      "pad_token_id": null,
      "prefix": null,
      "problem_type": null,
      "pruned_heads": {},
      "remove_invalid_values": false,
      "repetition_penalty": 1.0,
      "return_dict": true,
      "return_dict_in_generate": false,
      "rms_norm_eps": 1e-06,
      "rope_scaling": {
        "mrope_section": [
          16,
          24,
          24
        ],
        "rope_type": "default",
        "type": "default"
      },
      "rope_theta": 1000000.0,
      "sep_token_id": null,
      "sliding_window": 32768,
      "suppress_tokens": null,
      "task_specific_params": null,
      "temperature": 1.0,
      "tf_legacy_loss": false,
      "tie_encoder_decoder": false,
      "tie_word_embeddings": false,
      "tokenizer_class": null,
      "top_k": 50,
      "top_p": 1.0,
      "torch_dtype": null,
      "torchscript": false,
      "typical_p": 1.0,
      "use_bfloat16": false,
      "use_cache": true,
      "use_sliding_window": false,
      "vocab_size": 152064
    },
    "torch_dtype": "bfloat16",
    "user_token_id": 872,
    "video_token_index": 151656,
    "vision_config": {
      "_name_or_path": "",
      "add_cross_attention": false,
      "architectures": null,
      "bad_words_ids": null,
      "begin_suppress_tokens": null,
      "bos_token_id": null,
      "chunk_size_feed_forward": 0,
      "cross_attention_hidden_size": null,
      "decoder_start_token_id": null,
      "depth": 32,
      "diversity_penalty": 0.0,
      "do_sample": false,
      "early_stopping": false,
      "embed_dim": 1280,
      "encoder_no_repeat_ngram_size": 0,
      "eos_token_id": null,
      "exponential_decay_length_penalty": null,
      "finetuning_task": null,
      "forced_bos_token_id": null,
      "forced_eos_token_id": null,
      "fullatt_block_indexes": [
        7,
        15,
        23,
        31
      ],
      "hidden_act": "silu",
      "hidden_size": 1280,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "in_channels": 3,
      "in_chans": 3,
      "init_std": 0.02,
      "initializer_range": 0.02,
      "intermediate_size": 3420,
      "is_decoder": false,
      "is_encoder_decoder": false,
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "length_penalty": 1.0,
      "max_length": 20,
      "min_length": 0,
      "model_type": "qwen2_5_omni_vision_encoder",
      "no_repeat_ngram_size": 0,
      "num_beam_groups": 1,
      "num_beams": 1,
      "num_heads": 16,
      "num_return_sequences": 1,
      "out_hidden_size": 3584,
      "output_attentions": false,
      "output_hidden_states": false,
      "output_scores": false,
      "pad_token_id": null,
      "patch_size": 14,
      "prefix": null,
      "problem_type": null,
      "pruned_heads": {},
      "remove_invalid_values": false,
      "repetition_penalty": 1.0,
      "return_dict": true,
      "return_dict_in_generate": false,
      "sep_token_id": null,
      "spatial_merge_size": 2,
      "spatial_patch_size": 14,
      "suppress_tokens": null,
      "task_specific_params": null,
      "temperature": 1.0,
      "temporal_patch_size": 2,
      "tf_legacy_loss": false,
      "tie_encoder_decoder": false,
      "tie_word_embeddings": true,
      "tokenizer_class": null,
      "tokens_per_second": 25,
      "top_k": 50,
      "top_p": 1.0,
      "torch_dtype": null,
      "torchscript": false,
      "typical_p": 1.0,
      "use_bfloat16": false,
      "window_size": 112
    },
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654
  },
  "token2wav_config": {
    "bigvgan_config": {
      "_name_or_path": "",
      "add_cross_attention": false,
      "architectures": null,
      "bad_words_ids": null,
      "begin_suppress_tokens": null,
      "bos_token_id": null,
      "chunk_size_feed_forward": 0,
      "cross_attention_hidden_size": null,
      "decoder_start_token_id": null,
      "diversity_penalty": 0.0,
      "do_sample": false,
      "early_stopping": false,
      "encoder_no_repeat_ngram_size": 0,
      "eos_token_id": null,
      "exponential_decay_length_penalty": null,
      "finetuning_task": null,
      "forced_bos_token_id": null,
      "forced_eos_token_id": null,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "is_decoder": false,
      "is_encoder_decoder": false,
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "length_penalty": 1.0,
      "max_length": 20,
      "mel_dim": 80,
      "min_length": 0,
      "model_type": "qwen2_5_omni_bigvgan",
      "no_repeat_ngram_size": 0,
      "num_beam_groups": 1,
      "num_beams": 1,
      "num_return_sequences": 1,
      "output_attentions": false,
      "output_hidden_states": false,
      "output_scores": false,
      "pad_token_id": null,
      "prefix": null,
      "problem_type": null,
      "pruned_heads": {},
      "remove_invalid_values": false,
      "repetition_penalty": 1.0,
      "resblock_dilation_sizes": [
        [
          1,
          3,
          5
        ],
        [
          1,
          3,
          5
        ],
        [
          1,
          3,
          5
        ]
      ],
      "resblock_kernel_sizes": [
        3,
        7,
        11
      ],
      "return_dict": true,
      "return_dict_in_generate": false,
      "sep_token_id": null,
      "suppress_tokens": null,
      "task_specific_params": null,
      "temperature": 1.0,
      "tf_legacy_loss": false,
      "tie_encoder_decoder": false,
      "tie_word_embeddings": true,
      "tokenizer_class": null,
      "top_k": 50,
      "top_p": 1.0,
      "torch_dtype": null,
      "torchscript": false,
      "typical_p": 1.0,
      "upsample_initial_channel": 1536,
      "upsample_kernel_sizes": [
        11,
        7,
        4,
        4,
        4,
        4
      ],
      "upsample_rates": [
        5,
        3,
        2,
        2,
        2,
        2
      ],
      "use_bfloat16": false,
      "use_bias_at_final": false
    },
    "dit_config": {
      "_name_or_path": "",
      "add_cross_attention": false,
      "architectures": null,
      "bad_words_ids": null,
      "begin_suppress_tokens": null,
      "block_size": 24,
      "bos_token_id": null,
      "chunk_size_feed_forward": 0,
      "cross_attention_hidden_size": null,
      "decoder_start_token_id": null,
      "depth": 22,
      "dim": 1024,
      "diversity_penalty": 0.0,
      "do_sample": false,
      "dropout": 0.1,
      "early_stopping": false,
      "emb_dim": 512,
      "enc_attention_channels": 64,
      "enc_channels": [
        256,
        256,
        256,
        256,
        768
      ],
      "enc_dilations": [
        1,
        2,
        3,
        4,
        1
      ],
      "enc_dim": 128,
      "enc_emb_dim": 192,
      "enc_global_context": true,
      "enc_kernel_sizes": [
        5,
        3,
        3,
        3,
        1
      ],
      "enc_lin_neurons": 192,
      "enc_res2net_scale": 2,
      "enc_se_channels": 64,
      "encoder_no_repeat_ngram_size": 0,
      "eos_token_id": null,
      "exponential_decay_length_penalty": null,
      "ff_mult": 2,
      "finetuning_task": null,
      "forced_bos_token_id": null,
      "forced_eos_token_id": null,
      "head_dim": 64,
      "heads": 16,
      "hidden_size": 1024,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "is_decoder": false,
      "is_encoder_decoder": false,
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "length_penalty": 1.0,
      "look_ahead_layers": [
        10
      ],
      "look_backward_layers": [
        0,
        20
      ],
      "max_length": 20,
      "max_position_embeddings": 32768,
      "mel_dim": 80,
      "min_length": 0,
      "model_type": "qwen2_5_omni_dit",
      "no_repeat_ngram_size": 0,
      "num_attention_heads": 16,
      "num_beam_groups": 1,
      "num_beams": 1,
      "num_embeds": 8193,
      "num_hidden_layers": 22,
      "num_return_sequences": 1,
      "output_attentions": false,
      "output_hidden_states": false,
      "output_scores": false,
      "pad_token_id": null,
      "prefix": null,
      "problem_type": null,
      "pruned_heads": {},
      "remove_invalid_values": false,
      "repeats": 2,
      "repetition_penalty": 1.0,
      "return_dict": true,
      "return_dict_in_generate": false,
      "rope_theta": 10000.0,
      "sep_token_id": null,
      "suppress_tokens": null,
      "task_specific_params": null,
      "temperature": 1.0,
      "tf_legacy_loss": false,
      "tie_encoder_decoder": false,
      "tie_word_embeddings": true,
      "tokenizer_class": null,
      "top_k": 50,
      "top_p": 1.0,
      "torch_dtype": "float32",
      "torchscript": false,
      "typical_p": 1.0,
      "use_bfloat16": false
    },
    "model_type": "qwen2_5_omni_token2wav"
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.2"
}

[INFO|2025-10-22 11:56:57] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
🚨 Config not found for qwen2-5-omni-backup. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py
[INFO|modeling_utils.py:1147] 2025-10-22 11:56:57,781 >> loading weights file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/hf_cache/models--Qwen--Qwen2.5-Omni-7B/snapshots/ae9e1690543ffd5c0221dc27f79834d0294cba00/model.safetensors.index.json
[INFO|modeling_utils.py:2240] 2025-10-22 11:56:57,782 >> Instantiating Qwen2_5OmniForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-10-22 11:56:57,786 >> Generate config GenerationConfig {
  "use_cache": false
}

[INFO|configuration_utils.py:1135] 2025-10-22 11:56:57,787 >> Generate config GenerationConfig {
  "bos_token_id": 151644,
  "eos_token_id": 151645,
  "pad_token_id": 151643
}

[INFO|configuration_utils.py:1135] 2025-10-22 11:56:57,830 >> Generate config GenerationConfig {}

[WARNING|logging.py:328] 2025-10-22 11:56:57,841 >> Qwen2_5OmniToken2WavModel does not support eager attention implementation, fall back to sdpa
[INFO|modeling_utils.py:2240] 2025-10-22 11:56:57,841 >> Instantiating Qwen2_5OmniToken2WavDiTModel model under default dtype torch.float32.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:05,  1.39s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:02<00:09,  2.29s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.06s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:07,  1.82s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:04,  1.47s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:02<00:10,  2.54s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:01,  1.03it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:04,  1.36s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:05,  1.72s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.05it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:04<00:03,  1.56s/it]
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
[rank3]:     return _run_exp()  # use absolute import
[rank3]:            ^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 52, in run_sft
[rank3]:     model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/model/loader.py", line 173, in load_model
[rank3]:     model = load_class.from_pretrained(**init_kwargs)
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
[rank3]:     return model_class.from_pretrained(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni_backup.py", line 4909, in from_pretrained
[rank3]:     model = super().from_pretrained(
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 309, in _wrapper
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 4573, in from_pretrained
[rank3]:     ) = cls._load_pretrained_model(
[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 5030, in _load_pretrained_model
[rank3]:     disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
[rank3]:                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 805, in _load_state_dict_into_meta_model
[rank3]:     param = param[...]
[rank3]:             ~~~~~^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 3 has a total capacity of 79.14 GiB of which 219.19 MiB is free. Process 3088267 has 62.63 GiB memory in use. Including non-PyTorch memory, this process has 16.28 GiB memory in use. Of the allocated memory 15.87 GiB is allocated by PyTorch, and 5.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:02,  1.23s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.05it/s]
Loading checkpoint shards:  60%|██████    | 3/5 [00:04<00:02,  1.46s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:05<00:01,  1.18s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:05<00:03,  1.81s/it]
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
[rank2]:     return _run_exp()  # use absolute import
[rank2]:            ^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 52, in run_sft
[rank2]:     model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/model/loader.py", line 173, in load_model
[rank2]:     model = load_class.from_pretrained(**init_kwargs)
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
[rank2]:     return model_class.from_pretrained(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni_backup.py", line 4909, in from_pretrained
[rank2]:     model = super().from_pretrained(
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 309, in _wrapper
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 4573, in from_pretrained
[rank2]:     ) = cls._load_pretrained_model(
[rank2]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 5030, in _load_pretrained_model
[rank2]:     disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
[rank2]:                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/modeling_utils.py", line 805, in _load_state_dict_into_meta_model
[rank2]:     param = param[...]
[rank2]:             ~~~~~^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 2 has a total capacity of 79.14 GiB of which 219.19 MiB is free. Process 3088266 has 62.63 GiB memory in use. Including non-PyTorch memory, this process has 16.28 GiB memory in use. Of the allocated memory 15.87 GiB is allocated by PyTorch, and 5.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1022 11:57:03.933000 3091730 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3091796 closing signal SIGTERM
W1022 11:57:03.937000 3091730 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3091797 closing signal SIGTERM
W1022 11:57:03.940000 3091730 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3091798 closing signal SIGTERM
E1022 11:57:04.405000 3091730 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 3 (pid: 3091799) of binary: /home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/bin/python
Traceback (most recent call last):
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-22_11:57:03
  host      : sruk-sbb48
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3091799)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/bin/llamafactory-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/cli.py", line 130, in main
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '37237', '/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py', '/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/qwen2_5_full_sft.yaml']' returned non-zero exit status 1.
