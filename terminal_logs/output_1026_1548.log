[INFO|2025-10-26 15:48:56] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:54355
W1026 15:49:01.375000 3463189 site-packages/torch/distributed/run.py:766] 
W1026 15:49:01.375000 3463189 site-packages/torch/distributed/run.py:766] *****************************************
W1026 15:49:01.375000 3463189 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 15:49:01.375000 3463189 site-packages/torch/distributed/run.py:766] *****************************************
[2025-10-26 15:49:07,437] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 15:49:07,724] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[WARNING|2025-10-26 15:49:08] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-10-26 15:49:08,769] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 15:49:09,004] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-26 15:49:09,133] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 15:49:09,286] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[INFO|2025-10-26 15:49:09] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-26 15:49:09] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[2025-10-26 15:49:10,380] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[INFO|2025-10-26 15:49:10] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-10-26 15:49:10] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:10,768 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:10,768 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:10,768 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:10,768 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:10,768 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:10,768 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:10,768 >> loading file chat_template.jinja
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|tokenization_utils_base.py:2299] 2025-10-26 15:49:11,079 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-10-26 15:49:11,079 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-10-26 15:49:11,082 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-10-26 15:49:11,082 >> Image processor Qwen2VLImageProcessor {
  "chunk_length": 300,
  "dither": 0.0,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|video_processing_utils.py:627] 2025-10-26 15:49:11,083 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-10-26 15:49:11,083 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "chunk_length": 300,
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "dither": 0.0,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|feature_extraction_utils.py:548] 2025-10-26 15:49:11,087 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/preprocessor_config.json
[INFO|feature_extraction_utils.py:597] 2025-10-26 15:49:11,088 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 300,
  "dither": 0.0,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:11,088 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:11,088 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:11,088 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:11,088 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:11,088 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:11,088 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 15:49:11,088 >> loading file chat_template.jinja
[2025-10-26 15:49:11,123] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][INFO|tokenization_utils_base.py:2299] 2025-10-26 15:49:11,395 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][INFO|processing_utils.py:990] 2025-10-26 15:49:11,805 >> Processor Qwen2_5OmniProcessor:
- image_processor: Qwen2VLImageProcessor {
  "chunk_length": 300,
  "dither": 0.0,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "chunk_length": 300,
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "dither": 0.0,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

- feature_extractor: WhisperFeatureExtractor {
  "chunk_length": 300,
  "dither": 0.0,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|AUDIO|>', '<|audio_bos|>', '<|audio_eos|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_bos|>', '<|vision_eos|>', '<|vision_pad|>', '<|IMAGE|>', '<|VIDEO|>'], 'audio_bos_token': '<|audio_bos|>', 'audio_eos_token': '<|audio_eos|>', 'audio_token': '<|AUDIO|>', 'image_token': '<|IMAGE|>', 'video_token': '<|VIDEO|>', 'vision_bos_token': '<|vision_bos|>', 'vision_eos_token': '<|vision_eos|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|AUDIO|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|audio_bos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|audio_eos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_bos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_eos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|IMAGE|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|VIDEO|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)

{
  "processor_class": "Qwen2_5OmniProcessor"
}

[INFO|2025-10-26 15:49:11] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[WARNING|2025-10-26 15:49:11] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
[INFO|2025-10-26 15:49:11] llamafactory.data.loader:143 >> Loaded tokenized dataset from /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/saves/datasets/lemon_tok_192.
[INFO|configuration_utils.py:696] 2025-10-26 15:49:11,846 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/config.json
[WARNING|modeling_rope_utils.py:467] 2025-10-26 15:49:11,847 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|configuration_utils.py:770] 2025-10-26 15:49:11,849 >> Model config Qwen2_5OmniThinkerConfig {
  "architectures": [
    "Qwen2_5OmniThinkerForConditionalGeneration"
  ],
  "audio_config": {
    "activation_dropout": 0.0,
    "activation_function": "gelu",
    "attention_dropout": 0.0,
    "d_model": 1280,
    "dropout": 0.0,
    "encoder_attention_heads": 20,
    "encoder_ffn_dim": 5120,
    "encoder_layerdrop": 0.0,
    "encoder_layers": 32,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "max_source_positions": 1500,
    "model_type": "qwen2_5_omni_audio_encoder",
    "n_window": 100,
    "num_hidden_layers": 32,
    "num_mel_bins": 128,
    "output_dim": 3584,
    "scale_embedding": false
  },
  "audio_end_token_id": 151648,
  "audio_start_token_id": 151647,
  "audio_token_index": 151646,
  "bos_token_id": 151644,
  "eos_token_id": 151645,
  "ignore_index": -100,
  "image_token_index": 151655,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "model_type": "qwen2_5_omni_thinker",
  "pad_token_id": 151643,
  "position_id_per_seconds": 25,
  "seconds_per_chunk": 0.4,
  "text_config": {
    "attention_dropout": 0.0,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_5_omni_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 152064
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.2",
  "user_token_id": 872,
  "video_token_index": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_omni_vision_encoder",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 25,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654
}

[INFO|2025-10-26 15:49:11] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|modeling_utils.py:1147] 2025-10-26 15:49:12,041 >> loading weights file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/model.safetensors.index.json
[INFO|modeling_utils.py:2240] 2025-10-26 15:49:12,042 >> Instantiating Qwen2_5OmniThinkerForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-10-26 15:49:12,045 >> Generate config GenerationConfig {
  "bos_token_id": 151644,
  "eos_token_id": 151645,
  "pad_token_id": 151643
}

[INFO|modeling_utils.py:2240] 2025-10-26 15:49:12,045 >> Instantiating Qwen2_5OmniAudioEncoder model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2240] 2025-10-26 15:49:12,063 >> Instantiating Qwen2_5OmniVisionEncoder model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2240] 2025-10-26 15:49:12,077 >> Instantiating Qwen2_5OmniThinkerTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.39s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.41s/it]Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
[INFO|modeling_utils.py:5130] 2025-10-26 15:49:16,028 >> All model checkpoint weights were used when initializing Qwen2_5OmniThinkerForConditionalGeneration.

[INFO|modeling_utils.py:5138] 2025-10-26 15:49:16,029 >> All the weights of Qwen2_5OmniThinkerForConditionalGeneration were initialized from the model checkpoint at /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5OmniThinkerForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-10-26 15:49:16,033 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/generation_config.json
[INFO|configuration_utils.py:1135] 2025-10-26 15:49:16,034 >> Generate config GenerationConfig {}

[INFO|2025-10-26 15:49:16] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-10-26 15:49:16] llamafactory.model.loader:143 >> all params: 8,931,813,888
[INFO|trainer.py:756] 2025-10-26 15:49:16,073 >> Using auto half precision backend
[WARNING|2025-10-26 15:49:16] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4327] 2025-10-26 15:49:16,088 >> 
***** Running Prediction *****
[INFO|trainer.py:4329] 2025-10-26 15:49:16,088 >>   Num examples = 1206
[INFO|trainer.py:4332] 2025-10-26 15:49:16,088 >>   Batch size = 1
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:49:42,532 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:49:42,532 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/301 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:50:04,068 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:50:04,068 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
  1%|          | 2/301 [00:16<40:27,  8.12s/it][WARNING|configuration_utils.py:839] 2025-10-26 15:50:13,523 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:50:13,523 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  1%|          | 3/301 [00:32<56:57, 11.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:50:54,591 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:50:54,591 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
  1%|▏         | 4/301 [01:07<1:40:02, 20.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:51:11,132 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:51:11,132 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
  2%|▏         | 5/301 [01:23<1:33:23, 18.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:51:21,975 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:51:21,975 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  2%|▏         | 6/301 [01:37<1:23:55, 17.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:51:34,539 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:51:34,539 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  2%|▏         | 7/301 [01:48<1:14:03, 15.12s/it][WARNING|configuration_utils.py:839] 2025-10-26 15:51:45,237 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:51:45,237 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  3%|▎         | 8/301 [02:04<1:15:05, 15.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:52:01,518 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:52:01,518 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  3%|▎         | 9/301 [02:20<1:16:02, 15.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:52:20,137 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:52:20,138 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  3%|▎         | 10/301 [02:49<1:35:21, 19.66s/it][WARNING|configuration_utils.py:839] 2025-10-26 15:52:41,575 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:52:41,575 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  4%|▎         | 11/301 [03:04<1:28:22, 18.28s/it][WARNING|configuration_utils.py:839] 2025-10-26 15:53:01,237 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:53:01,237 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  4%|▍         | 12/301 [03:21<1:26:28, 17.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:53:24,975 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:53:24,976 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
  4%|▍         | 13/301 [03:37<1:23:48, 17.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:53:34,389 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:53:34,389 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  5%|▍         | 14/301 [03:54<1:23:05, 17.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:53:52,560 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:53:52,561 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  5%|▍         | 15/301 [04:11<1:21:39, 17.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 15:54:08,109 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:54:08,109 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  5%|▌         | 16/301 [04:28<1:21:45, 17.21s/it][WARNING|configuration_utils.py:839] 2025-10-26 15:54:26,233 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:54:26,233 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  6%|▌         | 17/301 [04:46<1:22:16, 17.38s/it][WARNING|configuration_utils.py:839] 2025-10-26 15:54:42,585 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 15:54:42,585 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
