[INFO|2025-10-26 15:58:58] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:36979
W1026 15:59:03.380000 3467145 site-packages/torch/distributed/run.py:766] 
W1026 15:59:03.380000 3467145 site-packages/torch/distributed/run.py:766] *****************************************
W1026 15:59:03.380000 3467145 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 15:59:03.380000 3467145 site-packages/torch/distributed/run.py:766] *****************************************
[2025-10-26 15:59:09,273] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 15:59:09,764] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[WARNING|2025-10-26 15:59:09] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-10-26 15:59:10,362] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 15:59:10,831] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-26 15:59:10,893] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 15:59:11,345] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
[rank3]:     run_exp()
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
[rank3]:     return _run_exp()  # use absolute import
[rank3]:            ^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 55, in _training_function
[rank3]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
[rank3]:                                                                              ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 219, in get_train_args
[rank3]:     model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
[rank3]:                                                                              ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 197, in _parse_train_args
[rank3]:     return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 79, in _parse_args
[rank3]:     return parser.parse_dict(args, allow_extra_keys=allow_extra_keys)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/hf_argparser.py", line 396, in parse_dict
[rank3]:     raise ValueError(f"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}")
[rank3]: ValueError: Some keys are not used by the HfArgumentParser: ['max_eval_samples']
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
[rank1]:     run_exp()
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
[rank1]:     return _run_exp()  # use absolute import
[rank1]:            ^^^^^^^^^^
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 55, in _training_function
[rank1]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
[rank1]:                                                                              ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 219, in get_train_args
[rank1]:     model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
[rank1]:                                                                              ^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 197, in _parse_train_args
[rank1]:     return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 79, in _parse_args
[rank1]:     return parser.parse_dict(args, allow_extra_keys=allow_extra_keys)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/hf_argparser.py", line 396, in parse_dict
[rank1]:     raise ValueError(f"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}")
[rank1]: ValueError: Some keys are not used by the HfArgumentParser: ['max_eval_samples']
[2025-10-26 15:59:11,956] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
[rank0]:     return _run_exp()  # use absolute import
[rank0]:            ^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 55, in _training_function
[rank0]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
[rank0]:                                                                              ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 219, in get_train_args
[rank0]:     model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
[rank0]:                                                                              ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 197, in _parse_train_args
[rank0]:     return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 79, in _parse_args
[rank0]:     return parser.parse_dict(args, allow_extra_keys=allow_extra_keys)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/hf_argparser.py", line 396, in parse_dict
[rank0]:     raise ValueError(f"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}")
[rank0]: ValueError: Some keys are not used by the HfArgumentParser: ['max_eval_samples']
[2025-10-26 15:59:12,559] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
[rank2]:     run_exp()
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
[rank2]:     return _run_exp()  # use absolute import
[rank2]:            ^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 55, in _training_function
[rank2]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
[rank2]:                                                                              ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 219, in get_train_args
[rank2]:     model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
[rank2]:                                                                              ^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 197, in _parse_train_args
[rank2]:     return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/hparams/parser.py", line 79, in _parse_args
[rank2]:     return parser.parse_dict(args, allow_extra_keys=allow_extra_keys)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/hf_argparser.py", line 396, in parse_dict
[rank2]:     raise ValueError(f"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}")
[rank2]: ValueError: Some keys are not used by the HfArgumentParser: ['max_eval_samples']
