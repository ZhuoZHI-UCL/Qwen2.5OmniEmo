[INFO|2025-10-26 16:26:51] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:49849
W1026 16:26:57.198000 3477179 site-packages/torch/distributed/run.py:766] 
W1026 16:26:57.198000 3477179 site-packages/torch/distributed/run.py:766] *****************************************
W1026 16:26:57.198000 3477179 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 16:26:57.198000 3477179 site-packages/torch/distributed/run.py:766] *****************************************
[WARNING|2025-10-26 16:27:03] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[2025-10-26 16:27:03,757] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:27:04,059] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:27:04,136] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:27:04,177] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:27:05,405] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-26 16:27:05,716] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-26 16:27:05,720] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[INFO|2025-10-26 16:27:05] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-10-26 16:27:05] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:05,802 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:05,802 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:05,802 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:05,802 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:05,802 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:05,802 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:05,802 >> loading file chat_template.jinja
[2025-10-26 16:27:05,809] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[INFO|tokenization_utils_base.py:2299] 2025-10-26 16:27:06,216 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:378] 2025-10-26 16:27:06,216 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/preprocessor_config.json
[INFO|image_processing_base.py:378] 2025-10-26 16:27:06,220 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/preprocessor_config.json
[INFO|image_processing_base.py:433] 2025-10-26 16:27:06,221 >> Image processor Qwen2VLImageProcessor {
  "chunk_length": 300,
  "dither": 0.0,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|video_processing_utils.py:627] 2025-10-26 16:27:06,221 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/video_preprocessor_config.json
[INFO|video_processing_utils.py:683] 2025-10-26 16:27:06,222 >> Video processor Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "chunk_length": 300,
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "dither": 0.0,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|feature_extraction_utils.py:548] 2025-10-26 16:27:06,227 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/preprocessor_config.json
[INFO|feature_extraction_utils.py:597] 2025-10-26 16:27:06,228 >> Feature extractor WhisperFeatureExtractor {
  "chunk_length": 300,
  "dither": 0.0,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:06,228 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:06,228 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:06,228 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:06,228 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:06,228 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:06,228 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-10-26 16:27:06,228 >> loading file chat_template.jinja
[INFO|2025-10-26 16:27:06] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-10-26 16:27:06] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2299] 2025-10-26 16:27:06,640 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-26 16:27:06] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|processing_utils.py:990] 2025-10-26 16:27:07,119 >> Processor Qwen2_5OmniProcessor:
- image_processor: Qwen2VLImageProcessor {
  "chunk_length": 300,
  "dither": 0.0,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- video_processor: Qwen2VLVideoProcessor {
  "_valid_kwargs_names": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "chunk_length": 300,
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "dither": 0.0,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "model_valid_processing_keys": [
    "do_convert_rgb",
    "do_resize",
    "size",
    "size_divisor",
    "default_to_square",
    "resample",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_pad",
    "do_center_crop",
    "crop_size",
    "data_format",
    "input_data_format",
    "device",
    "min_pixels",
    "max_pixels",
    "patch_size",
    "temporal_patch_size",
    "merge_size"
  ],
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

- feature_extractor: WhisperFeatureExtractor {
  "chunk_length": 300,
  "dither": 0.0,
  "feature_extractor_type": "WhisperFeatureExtractor",
  "feature_size": 128,
  "hop_length": 160,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "n_fft": 400,
  "n_samples": 4800000,
  "nb_max_frames": 30000,
  "padding_side": "right",
  "padding_value": 0.0,
  "patch_size": 14,
  "processor_class": "Qwen2_5OmniProcessor",
  "return_attention_mask": true,
  "sampling_rate": 16000,
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|AUDIO|>', '<|audio_bos|>', '<|audio_eos|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_bos|>', '<|vision_eos|>', '<|vision_pad|>', '<|IMAGE|>', '<|VIDEO|>'], 'audio_bos_token': '<|audio_bos|>', 'audio_eos_token': '<|audio_eos|>', 'audio_token': '<|AUDIO|>', 'image_token': '<|IMAGE|>', 'video_token': '<|VIDEO|>', 'vision_bos_token': '<|vision_bos|>', 'vision_eos_token': '<|vision_eos|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|AUDIO|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|audio_bos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|audio_eos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_bos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_eos|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|IMAGE|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|VIDEO|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)

{
  "processor_class": "Qwen2_5OmniProcessor"
}

[WARNING|2025-10-26 16:27:07] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
[INFO|2025-10-26 16:27:07] llamafactory.data.loader:143 >> Loaded tokenized dataset from /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/saves/datasets/lemon_tok_192_10samples.
[INFO|configuration_utils.py:696] 2025-10-26 16:27:07,161 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/config.json
[WARNING|modeling_rope_utils.py:467] 2025-10-26 16:27:07,161 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|configuration_utils.py:770] 2025-10-26 16:27:07,164 >> Model config Qwen2_5OmniThinkerConfig {
  "architectures": [
    "Qwen2_5OmniThinkerForConditionalGeneration"
  ],
  "audio_config": {
    "activation_dropout": 0.0,
    "activation_function": "gelu",
    "attention_dropout": 0.0,
    "d_model": 1280,
    "dropout": 0.0,
    "encoder_attention_heads": 20,
    "encoder_ffn_dim": 5120,
    "encoder_layerdrop": 0.0,
    "encoder_layers": 32,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "max_source_positions": 1500,
    "model_type": "qwen2_5_omni_audio_encoder",
    "n_window": 100,
    "num_hidden_layers": 32,
    "num_mel_bins": 128,
    "output_dim": 3584,
    "scale_embedding": false
  },
  "audio_end_token_id": 151648,
  "audio_start_token_id": 151647,
  "audio_token_index": 151646,
  "bos_token_id": 151644,
  "eos_token_id": 151645,
  "ignore_index": -100,
  "image_token_index": 151655,
  "init_std": 0.02,
  "initializer_range": 0.02,
  "model_type": "qwen2_5_omni_thinker",
  "pad_token_id": 151643,
  "position_id_per_seconds": 25,
  "seconds_per_chunk": 0.4,
  "text_config": {
    "attention_dropout": 0.0,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "max_position_embeddings": 32768,
    "max_window_layers": 28,
    "model_type": "qwen2_5_omni_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": 32768,
    "use_cache": true,
    "use_sliding_window": false,
    "vocab_size": 152064
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.2",
  "user_token_id": 872,
  "video_token_index": 151656,
  "vision_config": {
    "depth": 32,
    "embed_dim": 1280,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "init_std": 0.02,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_omni_vision_encoder",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 25,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654
}

[INFO|2025-10-26 16:27:07] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.
[INFO|modeling_utils.py:1147] 2025-10-26 16:27:07,357 >> loading weights file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/model.safetensors.index.json
[INFO|modeling_utils.py:2240] 2025-10-26 16:27:07,358 >> Instantiating Qwen2_5OmniThinkerForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-10-26 16:27:07,362 >> Generate config GenerationConfig {
  "bos_token_id": 151644,
  "eos_token_id": 151645,
  "pad_token_id": 151643
}

[INFO|modeling_utils.py:2240] 2025-10-26 16:27:07,363 >> Instantiating Qwen2_5OmniAudioEncoder model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2240] 2025-10-26 16:27:07,381 >> Instantiating Qwen2_5OmniVisionEncoder model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2240] 2025-10-26 16:27:07,395 >> Instantiating Qwen2_5OmniThinkerTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
[INFO|modeling_utils.py:5130] 2025-10-26 16:27:11,657 >> All model checkpoint weights were used when initializing Qwen2_5OmniThinkerForConditionalGeneration.

[INFO|modeling_utils.py:5138] 2025-10-26 16:27:11,657 >> All the weights of Qwen2_5OmniThinkerForConditionalGeneration were initialized from the model checkpoint at /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5OmniThinkerForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-10-26 16:27:11,662 >> loading configuration file /home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/output/lemon_omni_lora/merged/generation_config.json
[INFO|configuration_utils.py:1135] 2025-10-26 16:27:11,662 >> Generate config GenerationConfig {}

[INFO|2025-10-26 16:27:11] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-10-26 16:27:11] llamafactory.model.loader:143 >> all params: 8,931,813,888
[INFO|trainer.py:756] 2025-10-26 16:27:11,706 >> Using auto half precision backend
[WARNING|2025-10-26 16:27:11] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.
[INFO|trainer.py:4327] 2025-10-26 16:27:11,722 >> 
***** Running Prediction *****
[INFO|trainer.py:4329] 2025-10-26 16:27:11,722 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-10-26 16:27:11,722 >>   Batch size = 1
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 16:27:38,629 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 16:27:38,629 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/2 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[WARNING|configuration_utils.py:839] 2025-10-26 16:27:47,467 >> The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].
[INFO|configuration_utils.py:840] 2025-10-26 16:27:47,467 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
100%|██████████| 2/2 [00:03<00:00,  1.56s/it]Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Building prefix dict from the default dictionary ...
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model from cache /tmp/jieba.cache
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.553 seconds.
Prefix dict has been built successfully.
Loading model cost 0.556 seconds.
Prefix dict has been built successfully.
Loading model cost 0.560 seconds.
Prefix dict has been built successfully.
Loading model cost 0.574 seconds.
Prefix dict has been built successfully.
[INFO|integration_utils.py:832] 2025-10-26 16:27:51,242 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: Network error (ConnectTimeout), entering retry loop.
wandb: ERROR Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.
Traceback (most recent call last):
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1004, in init
    result = wait_with_progress(
             ^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/mailbox/wait_with_progress.py", line 23, in wait_with_progress
    return wait_all_with_progress(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/mailbox/wait_with_progress.py", line 77, in wait_all_with_progress
    return asyncer.run(progress_loop_with_timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py", line 136, in run
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
TimeoutError: Timed out waiting for response on fd5i7cwmm3pc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
    run_exp()
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
    return _run_exp()  # use absolute import
           ^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 129, in run_sft
    predict_results = trainer.predict(dataset_module["eval_dataset"], metric_key_prefix="predict", **gen_kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer_seq2seq.py", line 255, in predict
    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer.py", line 4268, in predict
    self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer_callback.py", line 541, in on_predict
    return self.call_event("on_predict", args, state, control, metrics=metrics)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer_callback.py", line 556, in call_event
    result = getattr(callback, event)(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/integrations/integration_utils.py", line 1024, in on_predict
    self.setup(args, state, **kwargs)
  File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/integrations/integration_utils.py", line 858, in setup
    self._wandb.init(
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1601, in init
    wandb._sentry.reraise(e)
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/analytics/sentry.py", line 162, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1582, in init
    run = wi.init(run_settings, run_config, run_printer)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1016, in init
    raise CommError(
wandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1004, in init
[rank0]:     result = wait_with_progress(
[rank0]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/mailbox/wait_with_progress.py", line 23, in wait_with_progress
[rank0]:     return wait_all_with_progress(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/mailbox/wait_with_progress.py", line 77, in wait_all_with_progress
[rank0]:     return asyncer.run(progress_loop_with_timeout)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py", line 136, in run
[rank0]:     return future.result()
[rank0]:            ^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/concurrent/futures/_base.py", line 456, in result
[rank0]:     return self.__get_result()
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[rank0]:     raise self._exception
[rank0]: TimeoutError: Timed out waiting for response on fd5i7cwmm3pc

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 57, in <module>
[rank0]:     run_exp()
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/launcher.py", line 41, in run_exp
[rank0]:     return _run_exp()  # use absolute import
[rank0]:            ^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 129, in run_sft
[rank0]:     predict_results = trainer.predict(dataset_module["eval_dataset"], metric_key_prefix="predict", **gen_kwargs)
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer_seq2seq.py", line 255, in predict
[rank0]:     return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer.py", line 4268, in predict
[rank0]:     self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer_callback.py", line 541, in on_predict
[rank0]:     return self.call_event("on_predict", args, state, control, metrics=metrics)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/trainer_callback.py", line 556, in call_event
[rank0]:     result = getattr(callback, event)(
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/integrations/integration_utils.py", line 1024, in on_predict
[rank0]:     self.setup(args, state, **kwargs)
[rank0]:   File "/home/CORP/zhuo.zhi/Project/Qwen2.5-Omni-EMO/transformers/integrations/integration_utils.py", line 858, in setup
[rank0]:     self._wandb.init(
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1601, in init
[rank0]:     wandb._sentry.reraise(e)
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/analytics/sentry.py", line 162, in reraise
[rank0]:     raise exc.with_traceback(sys.exc_info()[2])
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1582, in init
[rank0]:     run = wi.init(run_settings, run_config, run_printer)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/CORP/zhuo.zhi/miniconda3/envs/qwen2.5omniemo/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1016, in init
[rank0]:     raise CommError(
[rank0]: wandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.
